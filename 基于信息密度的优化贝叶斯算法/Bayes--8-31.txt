


%%%%%%%%%%%%%%%score  是train_feature的PCA结果   test_score是test_feature 的PCA结果

train_feature = score(:,1:6);
test_feature = test_score(:,1:6);
ObjBayes = NaiveBayes.fit(train_feature,train_class);
predict_label=ObjBayes.predict(test_feature);
accuracy         =       length(find(predict_label == test_class))/length(test_class)*100

%%%%%%%%%%%%%%用PCA后，分类结果是74%



%%%KNN 未使用PCA
train_feature = train_total(:,1:41);
 train_class = train_total(:,42);
 test_feature = test_data(:,1:41);
 test_class = test_data(:,42);
mdl = ClassificationKNN.fit(train_feature,train_class);
predict_label   =       predict(mdl, test_feature);
accuracy         =       length(find(predict_label == test_class))/length(test_class)*100

%%%accuracy = 94.9367

%%%%%鉴别分析分类器（discriminant analysis classifier）
train_feature_score = score(:,1:4);
test_feature_score = test_score(:,1:4);
obj = ClassificationDiscriminant.fit(train_feature_score,train_class);
predict_label   =   predict(obj, test_feature_score);
accuracy  = length(find(predict_label == test_class))/length(test_class)*100

%%%%%%%%%%%%%%%%%%%%%accuracy =82.3759


%%%%%%%%%%%%%%%%%%%%决策树算法   （不用PCA)%%%%
train_feature = train_total(:,1:41);
 train_class = train_total(:,42);
 test_feature = test_data(:,1:41);
 test_class = test_data(:,42);
 predict=treefit(train_feature,train_class);
predict_label   =       predict(test_feature);
accuracy         =       length(find(predict_label == test_class))/length(test_class)*100
%%%%%%%%%accuracy =95.5209


%%%%%%%%%%%%%%%%%%%%%%%支持向量机%%%%
SVMStruct = svmtrain(train_feature,train_class);
predict_label  = svmclassify(SVMStruct, train_class));


%%%%%%%%%%%%%%神经网络

[train_feature_inputn,train_feature_inputps]=mapminmax(train_feature)
pr(1:13296,1)=0; %输入矢量的取值范围矩阵
pr(1:13296,2)=1;
net=newff(pr,[41,12,5]);
net.trainParam.epochs=1000;%允许最大训练步数2000步
net.trainParam.goal=0.001; %训练目标最小误差0.001
net.trainParam.show=10; %每间隔100步显示一次训练结果
net.trainParam.lr=0.05; %学习速率0.05
net=train(net,train_feature_inputn,train_class)

%%%%%%lm算法   bp神经网络

train_feature = train_total(:,1:41);
 train_class = train_total(:,42);
 test_feature = test_data(:,1:41);
 test_class = test_data(:,42);

train_feature=train_feature';
train_class=train_class';
%%%%train_class=full(ind2vec(train_class));

test_feature=test_feature';
test_class=test_class';



net=patternnet(20,'trainlm');  %%20个中间单元
net.trainParam.epochs=1000;
net.trainParam.show=10;
net.trainParam.showCommandLine=0;
net.trainParam.showWindow=1;
net.trainParam.goal=0.001; 
net.trainParam.time=inf; 
net.trainParam.min_grad=0.000001; 
net.trainParam.max_fail=5%%%%%%检验次数
net.performFcn='mse';

net=train(net,train_feature,train_class);
net_train_class = sim(net,train_feature);


panduan =[];
for i =1:13296
      if   abs(net_train_class(i)-train_class(i))<=0.5;
           panduan(i)=1;
     else 
           panduan(i)=0;
     end
end
sum(panduan)
sum(panduan)/13296  %%%判断对了多少个



net_test_class = sim(net,test_feature);
panduan =[];
for i =1:1027
      if   abs(net_test_class(i)-test_class(i))<=0.5;
           panduan(i)=1;
     else 
           panduan(i)=0;
     end
end
sum(panduan)
sum(panduan)/1027 %%%判断对了多少个

 %%%%%%%%net.trainParam.max_fail=10 %%%%%%检验次数准确率  0.9523      978   没有加上cov   mean
 %%%%%%%%net.trainParam.max_fail=5 %%%%%%检验次数准确率  0.9221     947
 %%%%%%%%net.trainParam.max_fail=5 %%%%%%检验次数准确率     0.9299       955  加上了协方差cov（dWB） 有进步
 %%%%%%%%net.trainParam.max_fail=10 %%%%%%检验次数准确率     0.9299       955  加上了协方差cov（dWB） 有进步 训练次数减小


%9-15 空-5   13255   0.9969   971   0.9455
%mean -5       13255   0.9969   971   0.9455
%cov -10       13268     0.9979    975  0.9494
%mean -5    13265  0.9977   972   0.9464
%cov -10       13259     0.9972    971  0.9455
 


%%%%%%%%%%accuracy         =       length(find(predict_label == test_class))/length(test_class)*100





%%%%%%%%%%%%%%%matlab train函数%%%%%%%%%%%%%%%%%%%%%%%%

   [x,t] = simplefit_dataset
   net = feedforwardnet(10)
  net = train(net,x,t)
   y1 = sim(net,x)
    y2 = net(x)




sum_train_feature = [];
sum_train_feature =sum(train_feature,2);
sum_train_feature 里有4个数为0
41-4=37  

>> net.IW

ans = 

    [20x37 double]
    []
41-4=37 (41个输入减去4个为零的输入)
37个输入-----20个中间元

>> net.LW

ans = 

               []    []
    [1x20 double]    []

20个中间元---1个输出

740（37*20  输入----》输出）+20（中间---》输出）+21（bias）=781


一次探索实验

WB  

    1.3782
    1.3517
    1.2870
   -1.0744
    0.8777
   -0.5680
   -0.6703
   -0.4501
    0.3733
   -0.1354
   -0.0179
   -0.2666
    0.5029
   -0.6541
    0.7829
    0.8952
   -1.0146
   -1.2574
    1.3248
   -1.4059
   -0.4910
   -0.3423
   -0.0035
    0.1099
   -0.4269
   -0.0345
    0.8089
   -0.0039
   -0.1989
    0.0616
   -0.2719
   -0.1879
    0.4475
   -0.2212
    0.3267
    0.0617
   -0.3373
   -0.0690
    0.0281
   -0.1338
    0.2732
   -0.3503
   -0.9513
    0.2851
    0.0062
   -0.5557
    0.5936
    0.1276
    0.7318
   -0.3733
   -0.1092
   -0.1094
   -0.0728
   -0.2804
   -0.3581
   -0.6130
    0.2585
   -0.6116
    0.2953
    0.4207
    0.1530
   -0.3703
    0.1072
    0.0064
    0.0699
   -0.0829
    0.8109
   -0.6748
   -0.3708
    0.2811
   -0.1015
   -0.1500
   -1.4264
   -0.6378
   -0.3547
   -0.0625
    0.2752
   -0.2790
    0.2436
   -0.3990
    0.0356
   -0.1772
    0.1882
    0.3332
    0.0324
   -0.2369
   -0.8390
    0.9583
   -0.2115
   -0.2256
   -0.2274
   -0.3578
   -0.3716
    0.2274
    0.3565
   -0.1242
   -0.0106
   -0.3017
    0.2202
    0.4433
   -0.1083
   -0.2306
    0.0719
    0.2848
    0.1416
   -0.5682
    0.1920
   -0.4118
   -0.2047
    0.2322
   -0.1070
   -0.0012
   -0.1865
   -0.2538
   -0.2131
   -0.3516
   -0.2282
    0.2217
    0.4120
    0.4994
    0.0223
    0.1530
   -0.1202
   -0.2705
   -0.3865
   -0.2843
   -0.2177
   -0.2224
   -0.3813
    0.1917
   -0.1188
   -0.1873
   -0.4194
    0.1676
    0.1928
    0.2691
    0.0664
   -0.2651
   -0.2829
   -0.2447
   -0.9978
   -0.1737
    0.0977
    0.3229
    0.0739
   -0.1448
   -0.8347
   -0.2091
    0.4074
    0.3093
   -0.2623
    0.3775
    0.7780
   -0.0767
    0.1407
    0.0941
    0.2295
   -0.3921
   -1.0013
    0.5991
   -0.2168
   -0.2353
    0.0726
    0.1098
   -0.4354
   -0.5033
   -0.4998
   -0.0888
    0.1123
   -0.3058
   -0.0166
    0.2350
   -0.0492
   -0.0502
    0.1337
   -0.3005
   -0.2731
   -0.1091
    0.3292
   -0.3997
    0.3640
    0.0116
   -0.2151
    0.1953
    0.4552
    0.2578
   -0.7943
   -0.5571
    0.5324
    0.2576
   -0.0750
   -0.3315
   -0.8663
   -0.1430
   -0.3193
    0.2026
   -0.3625
    0.0893
   -0.0431
   -0.0331
    0.7199
    0.4076
   -0.2903
   -0.2392
   -0.2407
   -0.1017
    0.1110
    0.3343
   -0.5727
    0.0758
   -0.2335
   -0.2124
   -0.3626
    0.5555
   -0.0326
    0.0627
   -0.4102
   -0.1567
    0.4036
   -0.5507
    0.1119
    0.0502
    0.0445
   -0.7104
    0.0841
    0.8954
    0.4669
    0.0005
    0.0161
    0.1876
    0.5778
   -0.4039
   -0.5173
    0.8512
   -0.0773
    0.0720
   -0.2027
   -0.5576
    0.2298
    0.4008
    0.2058
   -0.2275
    0.1682
    0.1325
   -0.0761
   -0.3970
   -0.3021
   -0.2091
   -0.1877
    0.2307
   -0.2782
    0.4140
   -0.1440
    0.3997
   -0.2055
   -0.3173
   -0.3819
    0.1059
   -0.1116
   -0.0154
    0.1427
   -0.2274
   -0.0226
   -0.0861
    0.0326
    0.1583
   -0.7130
   -0.3974
    0.2373
    0.0172
    0.0743
   -0.1852
   -0.1740
   -0.1884
   -0.3848
    0.1102
   -0.4135
    0.6093
    0.2977
    0.0548
    0.4635
    0.0127
    0.2086
    0.3277
   -0.4325
   -0.3482
    0.2400
   -0.3443
    0.0405
    0.3116
    0.1548
   -0.2726
    0.0668
   -0.2819
   -0.3020
    0.1700
   -0.2989
    0.2541
   -0.3560
    0.0561
   -0.0459
   -0.3738
    0.3395
   -0.0883
   -0.4023
    0.1833
    0.3515
    0.2055
   -0.0742
    0.0341
   -0.4691
   -0.0462
   -0.3529
    0.3275
   -0.1229
   -0.2012
    0.1081
   -0.1938
   -0.2906
   -0.0654
    0.1123
   -0.0005
   -0.2483
    0.0463
   -0.2751
   -0.4214
   -0.2133
    0.2156
   -0.0601
   -0.2904
   -0.3895
    0.1185
   -0.4109
   -0.2906
   -0.1742
   -0.3858
    0.2307
    0.0442
    0.2142
    0.1345
    0.3332
   -0.2232
    0.1905
   -0.1649
   -0.1131
   -0.0434
   -0.1022
    0.4091
   -0.0439
   -0.3657
    0.1494
    0.0456
    0.2455
    0.4043
   -0.4080
    0.2167
    0.3422
   -0.2940
   -0.1871
    0.0217
   -0.4253
    0.3980
   -0.0695
   -0.3370
    0.2683
   -0.1220
    0.0936
   -0.6292
    0.2276
    0.2900
    0.0421
   -0.2741
   -0.0501
   -0.2455
   -0.2274
    0.1193
   -0.1393
    0.1456
    0.0156
   -0.0679
    0.1729
   -0.4185
    0.3209
   -0.1271
    0.0416
    0.1772
   -0.1788
    0.1070
   -0.2223
   -0.3099
    0.3800
   -0.3541
    0.2957
    0.0639
   -0.3731
    0.0618
   -0.2666
   -0.0538
    0.0239
    0.1406
   -0.2639
   -0.0627
   -0.2813
    0.1972
    0.0632
    0.4801
   -0.5104
   -0.0305
   -0.8836
    0.2829
   -0.1259
    0.3477
    0.2340
    0.2141
   -0.0682
    0.5755
   -0.0569
   -0.2194
    0.2216
    0.4671
    0.1700
   -0.2307
    0.2856
   -0.1821
    0.2661
   -0.2024
   -0.1072
    0.1733
    0.0831
    0.1436
   -0.1852
   -0.5225
    0.6503
   -0.1402
    0.0577
   -0.3230
   -0.1381
    0.3856
    0.4348
   -0.0923
    0.5820
   -0.4197
    0.1345
    0.4309
   -0.2929
    0.1972
    0.1341
    0.1329
   -0.3749
   -0.0729
    0.2217
   -0.0097
   -0.3094
   -0.4434
   -0.2776
   -0.2750
   -0.1840
   -0.0483
   -0.1881
   -0.2762
    0.0385
   -0.3399
    0.2793
    0.3952
   -0.0458
    0.0617
   -0.1874
    0.2697
   -0.4662
   -0.0560
   -0.3336
   -0.4491
   -0.0820
    0.0051
    0.0449
   -0.3187
    0.3115
    0.2997
    0.1221
   -0.2364
   -0.1262
    0.4321
    0.3318
   -0.5622
    0.0871
   -0.1582
    0.3815
   -0.1631
    0.2238
    0.2646
    0.2247
    0.3598
   -0.4038
   -0.4862
   -0.0488
    0.2901
   -0.2396
   -0.2179
   -0.1045
    0.0009
   -0.1565
    0.0780
   -0.1640
   -0.0985
    0.1379
    0.0172
   -0.2719
    0.4623
   -0.1694
   -0.0741
    0.2278
   -0.2652
   -0.9212
   -0.1989
    0.4681
   -0.1146
   -0.0225
    0.6540
   -0.0527
    0.3817
    0.1321
   -0.3275
    0.5516
   -0.1053
   -0.3853
   -0.2236
   -0.1951
   -0.0375
    0.3173
   -0.4335
    0.0676
   -0.1153
   -0.2873
    0.5782
    0.0970
    0.0107
   -0.0566
    0.2748
   -0.3283
   -0.3278
    0.0984
   -0.1695
    0.1712
    0.1706
    0.0864
    0.4250
    0.6367
    0.2840
   -0.2198
    0.0044
   -0.1065
   -0.0511
    0.1319
   -0.2718
   -0.1832
    0.2538
   -0.3847
    0.0821
   -0.0911
   -0.2539
    0.7949
    0.1443
    0.8932
    0.0918
   -0.2559
    0.6510
   -0.1980
    0.2921
   -0.2644
   -0.0255
    0.0531
   -0.4093
    0.7210
   -0.0119
   -0.3697
   -0.4739
    0.3523
   -0.1583
   -0.2047
   -0.6067
    1.5141
    0.0015
    0.4047
   -0.1129
    0.1435
   -0.6424
    1.5893
   -0.2193
   -0.0384
    0.0295
    0.3636
   -0.2871
   -0.1043
   -0.0299
   -0.4003
    0.3715
    0.0354
   -0.3642
   -0.3218
    0.8220
   -0.3188
   -0.3821
    0.4184
   -0.5178
   -0.3740
   -0.5869
   -0.2166
   -0.1151
    0.0137
   -0.0447
    0.3751
   -0.6664
   -0.8792
   -0.2466
   -0.0377
   -0.2421
    0.4002
    0.1770
    0.2397
   -0.6300
    0.0908
   -0.0899
    0.0368
    0.6910
    0.2050
    0.7508
    0.0173
   -0.5120
   -0.4605
   -0.0969
   -0.1988
   -0.0823
    0.3364
    0.5492
   -0.2954
    0.0997
   -0.3381
    0.2765
    0.2710
    0.0399
    0.7030
    0.0572
   -0.1859
    0.2705
    0.1268
    0.5615
   -1.1128
   -0.0567
   -0.2904
    0.0321
   -0.2514
   -0.2636
    0.7927
    0.0021
    0.0863
   -0.2099
   -0.0553
    0.3646
   -0.1089
    0.3431
   -0.5404
   -0.0357
   -0.0684
    0.0738
   -0.4384
   -0.3744
    0.1573
    0.0025
   -0.2458
   -0.3087
    0.2255
   -0.0017
    0.1623
    0.2261
    0.3085
    0.0267
    0.0014
   -0.4243
    0.1225
   -0.1839
    0.2090
    0.0054
    0.0389
    0.1370
    0.0534
   -0.3510
   -0.0162
   -0.1666
    0.1762
    0.2383
   -0.5555
   -0.1464
    0.1803
   -0.1545
   -0.1633
   -0.0196
    0.1047
    0.0244
    0.1256
   -0.2751
    0.0958
   -0.0863
    0.1439
   -0.3189
    0.3012
   -0.3852
   -0.0395
    0.0130
    0.4345
    0.3986
   -0.4820
    0.2062
   -0.0173
    0.2072
    0.0765
    0.0318
   -0.2316
   -0.2642
   -0.0848
    0.0106
    0.2742
    0.3054
    0.3247
    0.1373
    0.2306
    0.3766
   -0.3917
    0.1015
    0.5569
    0.7148
   -0.0078
   -0.3409
    0.0731
    0.1679
   -0.3599
   -0.3242
   -0.3965
    0.1896
    0.3955
   -0.3598
   -0.7396
   -0.0527
    0.0727
   -0.3094
   -0.0693
   -0.1914
    0.2739
   -0.1179
    0.0061
   -0.3221
   -0.5336
   -0.2944
   -0.2406
   -0.3572
   -0.3645
    0.0408
   -0.1297
    0.4474
    0.2366
    0.0979
    0.1674
    1.3753
    0.2140
   -1.0471
   -0.8919
    0.0937
   -1.2480
    2.0278
    1.3662
   -0.9238
   -0.1496
    0.0810
    1.0904
   -1.4647
    1.0113
    0.1950
   -0.2231
    0.1020
    0.7972
    0.7010
   -1.0855


ans =

   781     1


ans =

       13281


ans =

    0.9989


ans =

   945


ans =

    0.9202
















